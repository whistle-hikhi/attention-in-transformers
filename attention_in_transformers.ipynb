{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT2nUZm3WRdni3wz24fFQA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dZ-mhr-t3TKH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, d_model=2, row_dim=0, col_dim=1):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "    self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "    self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "    self.row_dim = row_dim\n",
        "    self.col_dim = col_dim\n",
        "\n",
        "  def forward(self, token_encodings):\n",
        "    q = self.W_q(token_encodings)\n",
        "    k = self.W_k(token_encodings)\n",
        "    v = self.W_v(token_encodings)\n",
        "\n",
        "    sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
        "    sims_scaled = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
        "\n",
        "    attention_percentages = F.softmax(sims_scaled, dim=self.col_dim)\n",
        "    attention_scores = torch.matmul(attention_percentages, v)\n",
        "\n",
        "    return attention_scores\n"
      ],
      "metadata": {
        "id": "uJVZ_iJ03Y95"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create a matrix of token encodings...\n",
        "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
        "                                 [0.57, 1.36],\n",
        "                                 [4.41, -2.16]])"
      ],
      "metadata": {
        "id": "XZ2RE9bn5E6H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## set the seed for the random number generator\n",
        "torch.manual_seed(42)\n",
        "\n",
        "## create a basic self-attention ojbect\n",
        "selfAttention = SelfAttention(d_model=2,\n",
        "                               row_dim=0,\n",
        "                               col_dim=1)\n",
        "\n",
        "## calculate basic attention for the token encodings\n",
        "selfAttention(encodings_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMAWh5pz5Hqs",
        "outputId": "648bc321-379d-40a3-d688-fa65c05e63f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0100, 1.0641],\n",
              "        [0.2040, 0.7057],\n",
              "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "62BU2J5i5L-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}